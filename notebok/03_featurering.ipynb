{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c710440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T03:10:30.397915Z",
     "start_time": "2021-08-13T03:10:30.395716Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3723634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T02:54:38.297463Z",
     "start_time": "2021-08-13T02:54:38.295413Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from src.utils import calc_wap, calc_wap2, log_return, realized_volatility, count_unique, rmspe, feval_RMSPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc78110",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3362f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # 実験番号\n",
    "    exp_no = 1\n",
    "    \n",
    "    random_seed = 42\n",
    "    input_dir = \"../input\"\n",
    "    output_dir = \"../output/data\"\n",
    "    \n",
    "    # preprocessor_book\n",
    "    feature_lst = [np.mean, np.sum, np.std, np.median, np.max, np.min]\n",
    "    feature_log_return = feature_lst + [realized_volatility]\n",
    "\n",
    "    create_feature_dict = {\n",
    "        \"log_return\": feature_log_return,\n",
    "        \"log_return2\": feature_log_return,\n",
    "        \"wap_balance\": feature_lst,\n",
    "        \"price_spread\": feature_lst,\n",
    "        \"bid_spread\": feature_lst,\n",
    "        \"ask_spread\": feature_lst,\n",
    "        \"volume_imbalance\": feature_lst,\n",
    "        \"total_volume\": feature_lst,\n",
    "        \"wap\": feature_lst,\n",
    "        \"wap2\": feature_lst,\n",
    "    }\n",
    "    \n",
    "    last_seconds = [150, 300, 450, ]\n",
    "    \n",
    "    # preprocessor_trade\n",
    "    aggregate_dictionary = {\n",
    "        \"log_return\": feature_log_return,\n",
    "        \"seconds_in_bucket\": [count_unique] + feature_lst,\n",
    "        \"size\": feature_lst,\n",
    "        \"order_count\": feature_lst,\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8a42a",
   "metadata": {},
   "source": [
    "# featureting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e397f89c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:29:13.793282Z",
     "start_time": "2021-08-12T14:29:13.787536Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor_book(file_path):\n",
    "    \"\"\"\n",
    "    bookデータの特徴量を生成\n",
    "    \n",
    "    CHECK\n",
    "    ------\n",
    "    CFG.create_feature_dict、CFG.last_secondsは外で定義してる\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # calculate return etc\n",
    "    df[\"wap\"] = calc_wap(df)\n",
    "    df[\"log_return\"] = df.groupby(\"time_id\")[\"wap\"].apply(log_return)\n",
    "\n",
    "    df[\"wap2\"] = calc_wap2(df)\n",
    "    df[\"log_return2\"] = df.groupby(\"time_id\")[\"wap2\"].apply(log_return)\n",
    "\n",
    "    df[\"wap_balance\"] = abs(df[\"wap\"] - df[\"wap2\"])\n",
    "\n",
    "    df[\"price_spread\"] = (df[\"ask_price1\"] - df[\"bid_price1\"]) / (\n",
    "        (df[\"ask_price1\"] + df[\"bid_price1\"]) / 2\n",
    "    )\n",
    "    df[\"bid_spread\"] = df[\"bid_price1\"] - df[\"bid_price2\"]\n",
    "    df[\"ask_spread\"] = df[\"ask_price1\"] - df[\"ask_price2\"]\n",
    "    df[\"total_volume\"] = (df[\"ask_size1\"] + df[\"ask_size2\"]) + (\n",
    "        df[\"bid_size1\"] + df[\"bid_size2\"]\n",
    "    )\n",
    "    df[\"volume_imbalance\"] = abs(\n",
    "        (df[\"ask_size1\"] + df[\"ask_size2\"]) - (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "    )\n",
    "\n",
    "    ##### groupby / all seconds\n",
    "    df_feature = pd.DataFrame(\n",
    "        df.groupby([\"time_id\"]).agg(CFG.create_feature_dict)\n",
    "    ).reset_index()\n",
    "\n",
    "    df_feature.columns = [\n",
    "        \"_\".join(col) for col in df_feature.columns\n",
    "    ]  # time_id is changed to time_id_\n",
    "\n",
    "    ###### groupby / last XX seconds\n",
    "    for second in CFG.last_seconds:\n",
    "        second = 600 - second\n",
    "\n",
    "        df_feature_sec = pd.DataFrame(\n",
    "            df.query(f\"seconds_in_bucket >= {second}\")\n",
    "            .groupby([\"time_id\"])\n",
    "            .agg(CFG.create_feature_dict)\n",
    "        ).reset_index()\n",
    "\n",
    "        df_feature_sec.columns = [\n",
    "            \"_\".join(col) for col in df_feature_sec.columns\n",
    "        ]  # time_id is changed to time_id_\n",
    "\n",
    "        df_feature_sec = df_feature_sec.add_suffix(\"_\" + str(second))\n",
    "\n",
    "        df_feature = pd.merge(\n",
    "            df_feature,\n",
    "            df_feature_sec,\n",
    "            how=\"left\",\n",
    "            left_on=\"time_id_\",\n",
    "            right_on=f\"time_id__{second}\",\n",
    "        )\n",
    "        df_feature = df_feature.drop([f\"time_id__{second}\"], axis=1)\n",
    "\n",
    "    # create row_id\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature = df_feature.drop([\"time_id_\"], axis=1)\n",
    "\n",
    "    return df_feature\n",
    "\n",
    "def preprocessor_trade(file_path):\n",
    "    \"\"\"\n",
    "    tradeデータの特徴量を生成\n",
    "\n",
    "    CHECK\n",
    "    ------\n",
    "    CFG.aggregate_dictionary、CFG.last_secondsは外で定義してる\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df[\"log_return\"] = df.groupby(\"time_id\")[\"price\"].apply(log_return)\n",
    "\n",
    "\n",
    "    df_feature = df.groupby(\"time_id\").agg(CFG.aggregate_dictionary)\n",
    "\n",
    "    df_feature = df_feature.reset_index()\n",
    "    df_feature.columns = [\"_\".join(col) for col in df_feature.columns]\n",
    "\n",
    "    ######groupby / last XX seconds\n",
    "    for second in CFG.last_seconds:\n",
    "        second = 600 - second\n",
    "\n",
    "        df_feature_sec = (\n",
    "            df.query(f\"seconds_in_bucket >= {second}\")\n",
    "            .groupby(\"time_id\")\n",
    "            .agg(CFG.aggregate_dictionary)\n",
    "        )\n",
    "        df_feature_sec = df_feature_sec.reset_index()\n",
    "\n",
    "        df_feature_sec.columns = [\"_\".join(col) for col in df_feature_sec.columns]\n",
    "        df_feature_sec = df_feature_sec.add_suffix(\"_\" + str(second))\n",
    "\n",
    "        df_feature = pd.merge(\n",
    "            df_feature,\n",
    "            df_feature_sec,\n",
    "            how=\"left\",\n",
    "            left_on=\"time_id_\",\n",
    "            right_on=f\"time_id__{second}\",\n",
    "        )\n",
    "        df_feature = df_feature.drop([f\"time_id__{second}\"], axis=1)\n",
    "\n",
    "    df_feature = df_feature.add_prefix(\"trade_\")\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"trade_time_id_\"].apply(\n",
    "        lambda x: f\"{stock_id}-{x}\"\n",
    "    )\n",
    "    df_feature = df_feature.drop([\"trade_time_id_\"], axis=1)\n",
    "\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "172edc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:29:13.802270Z",
     "start_time": "2021-08-12T14:29:13.799389Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor(input_dir, list_stock_ids, is_train=True):\n",
    "    from joblib import Parallel, delayed  # parallel computing to save time\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    BOOK_TRAIN = f\"book_train.parquet/stock_id={stock_id}\"\n",
    "    TRADE_TRAIN = f\"trade_train.parquet/stock_id={stock_id}\"\n",
    "    BOOK_TEST = f\"book_test.parquet/stock_id={stock_id}\"\n",
    "    TRADE_TEST = f\"trade_test.parquet/stock_id={stock_id}\"\n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "        if is_train:\n",
    "            file_path_book = os.path.join(\n",
    "                input_dir, BOOK_TRAIN\n",
    "            )\n",
    "            file_path_trade = os.path.join(\n",
    "                input_dir, TRADE_TRAIN\n",
    "            )\n",
    "        else:\n",
    "            file_path_book = os.path.join(\n",
    "                input_dir, BOOK_TEST\n",
    "            )\n",
    "            file_path_trade = os.path.join(\n",
    "                input_dir, TRADE_TEST\n",
    "            )\n",
    "\n",
    "        df_tmp = pd.merge(\n",
    "            preprocessor_book(file_path_book),\n",
    "            preprocessor_trade(file_path_trade),\n",
    "            on=\"row_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return pd.concat([df, df_tmp])\n",
    "\n",
    "    df = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n",
    "    )\n",
    "\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbe7c6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T02:54:06.529122Z",
     "start_time": "2021-08-13T02:54:06.525558Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_model_importance(model, feature_names=None, importance_type=\"gain\"):\n",
    "    importance_df = pd.DataFrame(\n",
    "        model.feature_importance(importance_type=importance_type),\n",
    "        index=feature_names,\n",
    "        columns=[\"importance\"],\n",
    "    ).sort_values(\"importance\")\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def plot_importance(importance_df, title=\"\", save_filepath=None, figsize=(8, 12)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    importance_df.plot.barh(ax=ax)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_filepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53691e42",
   "metadata": {},
   "source": [
    "# training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572efcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CHECK\n",
    "# book, tradeのパスglobで渡したい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2db62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:35:00.502785Z",
     "start_time": "2021-08-12T14:35:00.449473Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(CFG.input_dir, \"train.csv\"))\n",
    "train_ids = train.stock_id.unique()\n",
    "train[\"row_id\"] = train[\"stock_id\"].astype(str) + \"-\" + train[\"time_id\"].astype(str)\n",
    "train = train[[\"row_id\", \"target\"]]\n",
    "\n",
    "df_train = preprocessor(CFG.input_dir, list_stock_ids=train_ids, is_train=True)\n",
    "df_train = train.merge(df_train, on=[\"row_id\"], how=\"left\")\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995cfe0",
   "metadata": {},
   "source": [
    "# test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6cd3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:36:50.696306Z",
     "start_time": "2021-08-12T14:36:50.692320Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(CFG.input_dir, \"test.csv\"))\n",
    "test_ids = test.stock_id.unique()\n",
    "\n",
    "df_test = preprocessor(CFG.input_dir, list_stock_ids=test_ids, is_train=False)\n",
    "df_test = test.merge(df_test, on=[\"row_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92acc6b",
   "metadata": {},
   "source": [
    "# target encoding by stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fcddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:38:28.465096Z",
     "start_time": "2021-08-12T14:38:27.936604Z"
    }
   },
   "outputs": [],
   "source": [
    "# stock_id target encoding\n",
    "df_train[\"stock_id\"] = df_train[\"row_id\"].apply(lambda x: x.split(\"-\")[0])\n",
    "df_test[\"stock_id\"] = df_test[\"row_id\"].apply(lambda x: x.split(\"-\")[0])\n",
    "\n",
    "stock_id_target_mean = df_train.groupby(\"stock_id\")[\"target\"].mean()\n",
    "df_test[\"stock_id_target_enc\"] = df_test[\"stock_id\"].map(\n",
    "    stock_id_target_mean\n",
    ")  # test_set\n",
    "\n",
    "# training\n",
    "#### CHECK\n",
    "# この辺、あんまり分かってない\n",
    "# oofでtarget encordingしてるんだと思うけども\n",
    "# 自分で書き直したい\n",
    "tmp = np.repeat(np.nan, df_train.shape[0])\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=CFG.random_seed)\n",
    "for idx_1, idx_2 in kf.split(df_train):\n",
    "    target_mean = df_train.iloc[idx_1].groupby(\"stock_id\")[\"target\"].mean()\n",
    "\n",
    "    tmp[idx_2] = df_train[\"stock_id\"].iloc[idx_2].map(target_mean)\n",
    "df_train[\"stock_id_target_enc\"] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(os.path.join(CFG.output_dir, f\"train_exp{CFG.exp_no}.csv\"), index=False)\n",
    "df_test.to_csv(os.path.join(CFG.output_dir, f\"test_exp{CFG.exp_no}.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
