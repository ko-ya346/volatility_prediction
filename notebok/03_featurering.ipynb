{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9c710440",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T03:10:30.397915Z",
     "start_time": "2021-08-13T03:10:30.395716Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3723634",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T02:54:38.297463Z",
     "start_time": "2021-08-13T02:54:38.295413Z"
    }
   },
   "outputs": [],
   "source": [
    "sys.path.append(\"../\")\n",
    "from src.utils import calc_wap, calc_wap2, log_return, realized_volatility, count_unique, rmspe, feval_RMSPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc78110",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3362f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # 実験番号\n",
    "    exp_no = 1\n",
    "    \n",
    "    random_seed = 42\n",
    "    input_dir = \"../input\"\n",
    "    output_dir = \"../output/data\"\n",
    "    \n",
    "    # preprocessor_book\n",
    "    feature_lst = [np.mean, np.sum, np.std]\n",
    "    feature_log_return = [np.mean, np.sum, np.std. realized_volatility]\n",
    "\n",
    "    create_feature_dict = {\n",
    "        \"log_return\": feature_log_return,\n",
    "        \"log_return2\": feature_log_return,\n",
    "        \"wap_balance\": feature_lst,\n",
    "        \"price_spread\": feature_lst,\n",
    "        \"bid_spread\": feature_lst,\n",
    "        \"ask_spread\": feature_lst,\n",
    "        \"volume_imbalance\": feature_lst,\n",
    "        \"total_volume\": feature_lst,\n",
    "        \"wap\": feature_lst,\n",
    "        \"wap2\": feature_lst,\n",
    "    }\n",
    "    \n",
    "    last_seconds = [300]\n",
    "    \n",
    "    # preprocessor_trade\n",
    "    aggregate_dictionary = {\n",
    "        \"log_return\": feature_log_return,\n",
    "        \"seconds_in_bucket\": [count_unique] + feature_lst,\n",
    "        \"size\": feature_lst,\n",
    "        \"order_count\": feature_lst,\n",
    "    }\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8a42a",
   "metadata": {},
   "source": [
    "# featureting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e397f89c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:29:13.793282Z",
     "start_time": "2021-08-12T14:29:13.787536Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor_book(file_path):\n",
    "    \"\"\"\n",
    "    bookデータの特徴量を生成\n",
    "    \n",
    "    CHECK\n",
    "    ------\n",
    "    CFG.create_feature_dict、CFG.last_secondsは外で定義してる\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path)\n",
    "    \n",
    "    # calculate return etc\n",
    "    df[\"wap\"] = calc_wap(df)\n",
    "    df[\"log_return\"] = df.groupby(\"time_id\")[\"wap\"].apply(log_return)\n",
    "\n",
    "    df[\"wap2\"] = calc_wap2(df)\n",
    "    df[\"log_return2\"] = df.groupby(\"time_id\")[\"wap2\"].apply(log_return)\n",
    "\n",
    "    df[\"wap_balance\"] = abs(df[\"wap\"] - df[\"wap2\"])\n",
    "\n",
    "    df[\"price_spread\"] = (df[\"ask_price1\"] - df[\"bid_price1\"]) / (\n",
    "        (df[\"ask_price1\"] + df[\"bid_price1\"]) / 2\n",
    "    )\n",
    "    df[\"bid_spread\"] = df[\"bid_price1\"] - df[\"bid_price2\"]\n",
    "    df[\"ask_spread\"] = df[\"ask_price1\"] - df[\"ask_price2\"]\n",
    "    df[\"total_volume\"] = (df[\"ask_size1\"] + df[\"ask_size2\"]) + (\n",
    "        df[\"bid_size1\"] + df[\"bid_size2\"]\n",
    "    )\n",
    "    df[\"volume_imbalance\"] = abs(\n",
    "        (df[\"ask_size1\"] + df[\"ask_size2\"]) - (df[\"bid_size1\"] + df[\"bid_size2\"])\n",
    "    )\n",
    "\n",
    "    ##### groupby / all seconds\n",
    "    df_feature = pd.DataFrame(\n",
    "        df.groupby([\"time_id\"]).agg(CFG.create_feature_dict)\n",
    "    ).reset_index()\n",
    "\n",
    "    df_feature.columns = [\n",
    "        \"_\".join(col) for col in df_feature.columns\n",
    "    ]  # time_id is changed to time_id_\n",
    "\n",
    "    ###### groupby / last XX seconds\n",
    "    for second in CFG.last_seconds:\n",
    "        second = 600 - second\n",
    "\n",
    "        df_feature_sec = pd.DataFrame(\n",
    "            df.query(f\"seconds_in_bucket >= {second}\")\n",
    "            .groupby([\"time_id\"])\n",
    "            .agg(CFG.create_feature_dict)\n",
    "        ).reset_index()\n",
    "\n",
    "        df_feature_sec.columns = [\n",
    "            \"_\".join(col) for col in df_feature_sec.columns\n",
    "        ]  # time_id is changed to time_id_\n",
    "\n",
    "        df_feature_sec = df_feature_sec.add_suffix(\"_\" + str(second))\n",
    "\n",
    "        df_feature = pd.merge(\n",
    "            df_feature,\n",
    "            df_feature_sec,\n",
    "            how=\"left\",\n",
    "            left_on=\"time_id_\",\n",
    "            right_on=f\"time_id__{second}\",\n",
    "        )\n",
    "        df_feature = df_feature.drop([f\"time_id__{second}\"], axis=1)\n",
    "\n",
    "    # create row_id\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"time_id_\"].apply(lambda x: f\"{stock_id}-{x}\")\n",
    "    df_feature = df_feature.drop([\"time_id_\"], axis=1)\n",
    "\n",
    "    return df_feature\n",
    "\n",
    "def preprocessor_trade(file_path):\n",
    "    \"\"\"\n",
    "    tradeデータの特徴量を生成\n",
    "\n",
    "    CHECK\n",
    "    ------\n",
    "    CFG.aggregate_dictionary、CFG.last_secondsは外で定義してる\n",
    "    \"\"\"\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df[\"log_return\"] = df.groupby(\"time_id\")[\"price\"].apply(log_return)\n",
    "\n",
    "\n",
    "    df_feature = df.groupby(\"time_id\").agg(CFG.aggregate_dictionary)\n",
    "\n",
    "    df_feature = df_feature.reset_index()\n",
    "    df_feature.columns = [\"_\".join(col) for col in df_feature.columns]\n",
    "\n",
    "    ######groupby / last XX seconds\n",
    "    for second in CFG.last_seconds:\n",
    "        second = 600 - second\n",
    "\n",
    "        df_feature_sec = (\n",
    "            df.query(f\"seconds_in_bucket >= {second}\")\n",
    "            .groupby(\"time_id\")\n",
    "            .agg(CFG.aggregate_dictionary)\n",
    "        )\n",
    "        df_feature_sec = df_feature_sec.reset_index()\n",
    "\n",
    "        df_feature_sec.columns = [\"_\".join(col) for col in df_feature_sec.columns]\n",
    "        df_feature_sec = df_feature_sec.add_suffix(\"_\" + str(second))\n",
    "\n",
    "        df_feature = pd.merge(\n",
    "            df_feature,\n",
    "            df_feature_sec,\n",
    "            how=\"left\",\n",
    "            left_on=\"time_id_\",\n",
    "            right_on=f\"time_id__{second}\",\n",
    "        )\n",
    "        df_feature = df_feature.drop([f\"time_id__{second}\"], axis=1)\n",
    "\n",
    "    df_feature = df_feature.add_prefix(\"trade_\")\n",
    "    stock_id = file_path.split(\"=\")[1]\n",
    "    df_feature[\"row_id\"] = df_feature[\"trade_time_id_\"].apply(\n",
    "        lambda x: f\"{stock_id}-{x}\"\n",
    "    )\n",
    "    df_feature = df_feature.drop([\"trade_time_id_\"], axis=1)\n",
    "\n",
    "    return df_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "172edc2f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:29:13.802270Z",
     "start_time": "2021-08-12T14:29:13.799389Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessor(input_dir, list_stock_ids, is_train=True):\n",
    "    from joblib import Parallel, delayed  # parallel computing to save time\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    BOOK_TRAIN = f\"book_train.parquet/stock_id={stock_id}\"\n",
    "    TRADE_TRAIN = f\"trade_train.parquet/stock_id={stock_id}\"\n",
    "    BOOK_TEST = f\"book_test.parquet/stock_id={stock_id}\"\n",
    "    TRADE_TEST = f\"trade_test.parquet/stock_id={stock_id}\"\n",
    "    \n",
    "    def for_joblib(stock_id):\n",
    "        if is_train:\n",
    "            file_path_book = os.path.join(\n",
    "                input_dir, BOOK_TRAIN\n",
    "            )\n",
    "            file_path_trade = os.path.join(\n",
    "                input_dir, TRADE_TRAIN\n",
    "            )\n",
    "        else:\n",
    "            file_path_book = os.path.join(\n",
    "                input_dir, BOOK_TEST\n",
    "            )\n",
    "            file_path_trade = os.path.join(\n",
    "                input_dir, TRADE_TEST\n",
    "            )\n",
    "\n",
    "        df_tmp = pd.merge(\n",
    "            preprocessor_book(file_path_book),\n",
    "            preprocessor_trade(file_path_trade),\n",
    "            on=\"row_id\",\n",
    "            how=\"left\",\n",
    "        )\n",
    "        return pd.concat([df, df_tmp])\n",
    "\n",
    "    df = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n",
    "    )\n",
    "\n",
    "    df = pd.concat(df, ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cbe7c6fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T02:54:06.529122Z",
     "start_time": "2021-08-13T02:54:06.525558Z"
    }
   },
   "outputs": [],
   "source": [
    "def calc_model_importance(model, feature_names=None, importance_type=\"gain\"):\n",
    "    importance_df = pd.DataFrame(\n",
    "        model.feature_importance(importance_type=importance_type),\n",
    "        index=feature_names,\n",
    "        columns=[\"importance\"],\n",
    "    ).sort_values(\"importance\")\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def plot_importance(importance_df, title=\"\", save_filepath=None, figsize=(8, 12)):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    importance_df.plot.barh(ax=ax)\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    if save_filepath is None:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.savefig(save_filepath)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53691e42",
   "metadata": {},
   "source": [
    "# training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2db62",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:35:00.502785Z",
     "start_time": "2021-08-12T14:35:00.449473Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(CFG.input_dir, \"train.csv\"))\n",
    "train_ids = train.stock_id.unique()\n",
    "train[\"row_id\"] = train[\"stock_id\"].astype(str) + \"-\" + train[\"time_id\"].astype(str)\n",
    "train = train[[\"row_id\", \"target\"]]\n",
    "\n",
    "df_train = preprocessor(CFG.input_dir, list_stock_ids=train_ids, is_train=True)\n",
    "df_train = train.merge(df_train, on=[\"row_id\"], how=\"left\")\n",
    "# df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f995cfe0",
   "metadata": {},
   "source": [
    "# test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce6cd3f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:36:50.696306Z",
     "start_time": "2021-08-12T14:36:50.692320Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(os.path.join(CFG.input_dir, \"test.csv\"))\n",
    "test_ids = test.stock_id.unique()\n",
    "\n",
    "df_test = preprocessor(CFG.input_dir, list_stock_ids=test_ids, is_train=False)\n",
    "df_test = test.merge(df_test, on=[\"row_id\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92acc6b",
   "metadata": {},
   "source": [
    "# target encoding by stock_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44fcddc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:38:28.465096Z",
     "start_time": "2021-08-12T14:38:27.936604Z"
    }
   },
   "outputs": [],
   "source": [
    "# stock_id target encoding\n",
    "df_train[\"stock_id\"] = df_train[\"row_id\"].apply(lambda x: x.split(\"-\")[0])\n",
    "df_test[\"stock_id\"] = df_test[\"row_id\"].apply(lambda x: x.split(\"-\")[0])\n",
    "\n",
    "stock_id_target_mean = df_train.groupby(\"stock_id\")[\"target\"].mean()\n",
    "df_test[\"stock_id_target_enc\"] = df_test[\"stock_id\"].map(\n",
    "    stock_id_target_mean\n",
    ")  # test_set\n",
    "\n",
    "# training\n",
    "#### CHECK\n",
    "# この辺、あんまり分かってない\n",
    "# oofでtarget encordingしてるんだと思うけども\n",
    "# 自分で書き直したい\n",
    "tmp = np.repeat(np.nan, df_train.shape[0])\n",
    "kf = KFold(n_splits=10, shuffle=True, random_state=CFG.random_seed)\n",
    "for idx_1, idx_2 in kf.split(df_train):\n",
    "    target_mean = df_train.iloc[idx_1].groupby(\"stock_id\")[\"target\"].mean()\n",
    "\n",
    "    tmp[idx_2] = df_train[\"stock_id\"].iloc[idx_2].map(target_mean)\n",
    "df_train[\"stock_id_target_enc\"] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv(os.path.join(CFG.output_dir, f\"train_exp{CFG.exp_no}.csv\"), index=False)\n",
    "df_test.to_csv(os.path.join(CFG.output_dir, f\"test_exp{CFG.exp_no}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8058fc41",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a85897",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-12T14:39:05.838790Z",
     "start_time": "2021-08-12T14:39:05.836965Z"
    }
   },
   "outputs": [],
   "source": [
    "DO_FEAT_IMP = False\n",
    "if len(df_test) == 3:\n",
    "    DO_FEAT_IMP = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f2c9ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T02:53:22.152498Z",
     "start_time": "2021-08-13T02:53:22.121329Z"
    }
   },
   "outputs": [],
   "source": [
    "df_train['stock_id'] = df_train['stock_id'].astype(int)\n",
    "df_test['stock_id'] = df_test['stock_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62210408",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T03:10:16.373658Z",
     "start_time": "2021-08-13T03:10:16.345135Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7e33e99290a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'row_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_train' is not defined"
     ]
    }
   ],
   "source": [
    "X = df_train.drop(['row_id','target'],axis=1)\n",
    "y = df_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1955ea43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T02:57:20.539947Z",
     "start_time": "2021-08-13T02:57:20.534559Z"
    }
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "      \"objective\": \"rmse\", \n",
    "      \"metric\": \"rmse\", \n",
    "      \"boosting_type\": \"gbdt\",\n",
    "      'early_stopping_rounds': 30,\n",
    "      'learning_rate': 0.01,\n",
    "      'lambda_l1': 1,\n",
    "      'lambda_l2': 1,\n",
    "      'feature_fraction': 0.8,\n",
    "      'bagging_fraction': 0.8,\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573f6251",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T03:10:04.065405Z",
     "start_time": "2021-08-13T03:10:04.060323Z"
    }
   },
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, random_state=19901028, shuffle=True)\n",
    "oof = pd.DataFrame()                 # out-of-fold result\n",
    "models = []                          # models\n",
    "scores = 0.0                         # validation score\n",
    "\n",
    "gain_importance_list = []\n",
    "split_importance_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3f67e4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-13T03:10:18.238375Z",
     "start_time": "2021-08-13T03:10:18.139225Z"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n",
    "\n",
    "    print(\"Fold :\", fold+1)\n",
    "    \n",
    "    # create dataset\n",
    "    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n",
    "    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n",
    "    \n",
    "    #RMSPE weight\n",
    "    weights = 1/np.square(y_train)\n",
    "    lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)\n",
    "\n",
    "    weights = 1/np.square(y_valid)\n",
    "    lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n",
    "    \n",
    "    # model \n",
    "    model = lgbm.train(params=params,\n",
    "                      train_set=lgbm_train,\n",
    "                      valid_sets=[lgbm_train, lgbm_valid],\n",
    "                      num_boost_round=5000,         \n",
    "                      feval=feval_RMSPE,\n",
    "                      verbose_eval=100,\n",
    "                      categorical_feature = ['stock_id']                \n",
    "                     )\n",
    "    \n",
    "    # validation \n",
    "    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n",
    "\n",
    "    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n",
    "    print(f'Performance of the　prediction: , RMSPE: {RMSPE}')\n",
    "\n",
    "    #keep scores and models\n",
    "    scores += RMSPE / 5\n",
    "    models.append(model)\n",
    "    print(\"*\" * 100)\n",
    "    \n",
    "    # --- calc model feature importance ---\n",
    "    if DO_FEAT_IMP:    \n",
    "        feature_names = X_train.columns.values.tolist()\n",
    "        gain_importance_df = calc_model_importance(\n",
    "            model, feature_names=feature_names, importance_type='gain')\n",
    "        gain_importance_list.append(gain_importance_df)\n",
    "\n",
    "        split_importance_df = calc_model_importance(\n",
    "            model, feature_names=feature_names, importance_type='split')\n",
    "        split_importance_list.append(split_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318541d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_importance(importance_df_list):\n",
    "    mean_importance = np.mean(\n",
    "        np.array([df['importance'].values for df in importance_df_list]), axis=0)\n",
    "    mean_df = importance_df_list[0].copy()\n",
    "    mean_df['importance'] = mean_importance\n",
    "    return mean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9fde25",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DO_FEAT_IMP:\n",
    "    mean_gain_df = calc_mean_importance(gain_importance_list)\n",
    "    plot_importance(mean_gain_df, title='Model feature importance by gain')\n",
    "    mean_gain_df = mean_gain_df.reset_index().rename(columns={'index': 'feature_names'})\n",
    "    mean_gain_df.to_csv('gain_importance_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6c041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = df_test[['row_id']]\n",
    "X_test = df_test.drop(['time_id', 'row_id'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f6bb252d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-136b178e9c6a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#light gbm models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iteration\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_iteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "target = np.zeros(len(X_test))\n",
    "\n",
    "#light gbm models\n",
    "for model in models:\n",
    "    pred = model.predict(X_test[X_valid.columns], num_iteration=model.best_iteration)\n",
    "    target += pred / len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb8b78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = y_pred.assign(target = target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642aa818",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.to_csv('submission.csv',index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
